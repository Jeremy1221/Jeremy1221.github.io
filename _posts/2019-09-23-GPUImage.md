---
title: GPUImage
header: GPUImage
description: GPUImage
---

# You are firework, come on show them what you're worth.

[GPUImage 参考](https://blog.csdn.net/xoxo_x/article/details/52695032)

## Class

1. GPUImageFramebuffer.h
    // opengl 纹理 选项
    typedef struct GPUTextureOptions {
        GLenum minFilter;
        GLenum magFilter;
        GLenum wrapS;
        GLenum wrapT;
        GLenum internalFormat;
        GLenum format;
        GLenum type;
    } GPUTextureOptions;

    @property(readonly) CGSize size;
    @property(readonly) GPUTextureOptions textureOptions;
    @property(readonly) GLuint texture;
    @property(readonly) BOOL missingFramebuffer;

    // Initialization and teardown
    - (id)initWithSize:(CGSize)framebufferSize;
    - (id)initWithSize:(CGSize)framebufferSize textureOptions:(GPUTextureOptions)fboTextureOptions onlyTexture:(BOOL)onlyGenerateTexture;
    - (id)initWithSize:(CGSize)framebufferSize overriddenTexture:(GLuint)inputTexture;

    // Usage
    - (void)activateFramebuffer;

    // Reference counting
    - (void)lock;
    - (void)unlock;
    - (void)clearAllLocks;
    - (void)disableReferenceCounting;
    - (void)enableReferenceCounting;

    // Image capture
    - (CGImageRef)newCGImageFromFramebufferContents;
    - (void)restoreRenderTarget;

    // Raw data bytes
    - (void)lockForReading;
    - (void)unlockAfterReading;
    - (NSUInteger)bytesPerRow;
    - (GLubyte *)byteBuffer;

    .m

    {
        GLuint framebuffer;
    #if TARGET_IPHONE_SIMULATOR || TARGET_OS_IPHONE
        // CVPixelBufferRef 是一种像素图片类型 可以转换成UIImage 也可以转换成openGL Texture
        CVPixelBufferRef renderTarget;
        CVOpenGLESTextureRef renderTexture;
        NSUInteger readLockCount;
    #else
    #endif
        NSUInteger framebufferReferenceCount;
        BOOL referenceCountingDisabled;
    }

    - (void)generateFramebuffer;
    - (void)generateTexture;
    - (void)destroyFramebuffer;

    @end

    void dataProviderReleaseCallback (void *info, const void *data, size_t size);
    void dataProviderUnlockCallback (void *info, const void *data, size_t size);

    @implementation GPUImageFramebuffer

    @synthesize size = _size;
    @synthesize textureOptions = _textureOptions;
    @synthesize texture = _texture;
    @synthesize missingFramebuffer = _missingFramebuffer;

    #pragma mark -
    #pragma mark Initialization and teardown

    - (id)initWithSize:(CGSize)framebufferSize textureOptions:(GPUTextureOptions)fboTextureOptions onlyTexture:(BOOL)onlyGenerateTexture;
    {
        
    }

    - (id)initWithSize:(CGSize)framebufferSize overriddenTexture:(GLuint)inputTexture;
    {
        
    }

    - (id)initWithSize:(CGSize)framebufferSize;
    {
        
    }

    - (void)dealloc
    {
        [self destroyFramebuffer];
    }

    #pragma mark -
    #pragma mark Internal

    - (void)generateTexture;
    {
        
    }

    - (void)generateFramebuffer;
    {
        
    }

    - (void)destroyFramebuffer;
    {
        
    }

    #pragma mark -
    #pragma mark Usage

    - (void)activateFramebuffer;
    {
        
    }

    #pragma mark -
    #pragma mark Reference counting

    - (void)lock;
    {
        
    }

    - (void)unlock;
    {
        
    }

    - (void)clearAllLocks;
    {
        framebufferReferenceCount = 0;
    }

    - (void)disableReferenceCounting;
    {
        referenceCountingDisabled = YES;
    }

    - (void)enableReferenceCounting;
    {
        referenceCountingDisabled = NO;
    }

    #pragma mark -
    #pragma mark Image capture

    void dataProviderReleaseCallback (void *info, const void *data, size_t size)
    {
        free((void *)data);
    }

    void dataProviderUnlockCallback (void *info, const void *data, size_t size)
    {
    }

    - (CGImageRef)newCGImageFromFramebufferContents;
    {
        
    }

    - (void)restoreRenderTarget;
    {
    #if TARGET_IPHONE_SIMULATOR || TARGET_OS_IPHONE
        [self unlockAfterReading];
        CFRelease(renderTarget);
    #else
    #endif
    }

    #pragma mark -
    #pragma mark Raw data bytes

    - (void)lockForReading
    {
    
    }

    - (NSUInteger)bytesPerRow;
    {
        
    }

    - (GLubyte *)byteBuffer;
    {
   
    }

    - (GLuint)texture;
    {
    
    }


1. GPUImageOut.h
    
    // 判断当前线程是否是主线程 如果是 直接执行block 否则在主队列中执行
    void runOnMainQueueWithoutDeadlocking(void (^block)(void));
    // OS_OBJECT_USE_OBJC 如果为1 那就是6.0 之后ARC管理了dispatch_queue 
    // 获取GPUImageContext的queue 如果当前队列是 这个队列 直接执行 否则放到队列中执行
    // dispatch_queue_set_specific(queue1, queueKey1, &queueKey1, NULL)  dispatch_set_specific 取当前队列的key-value
    void runSynchronouslyOnVideoProcessingQueue(void (^block)(void));
    // 同上 不过是异步执行
    void runAsynchronouslyOnVideoProcessingQueue(void (^block)(void));
    // 同上 不过目标队列是指定的队列
    void runSynchronouslyOnContextQueue(GPUImageContext *context, void (^block)(void));
    // 同上
    void runAsynchronouslyOnContextQueue(GPUImageContext *context, void (^block)(void));
    /*
    如果tag 为 nil tag = "default"
    获取CPU 信息 然后输出 <mach/task_info.h>
    https://www.jianshu.com/p/22a077fd51f1
    */
    void reportAvailableMemoryForGPUImage(NSString *tag);

    implementation
    {
        GPUImageFramebuffer *outputFramebuffer;
        NSMUtableArray *targets, *targetTextureIndices;
        CGSize inputTextureSize, cachedMaximumOutputSize, forcedMaxiumSize;
        BOOL overrideInputSize;
        BOOL allTargetsWantMonochromeData;
        BOOL usingNextFrameForImageCaputre;
    }

    @property(readwrite, nonatomic) BOOL shouldSmoothlyScaleOutput;
    @property(readwrite, nonatomic) BOOL shouldIgnoreUpdatesToThisTarget;
    @property(readwrite, nonatomic, retain) GPUImageMovieWriter *audioEncodingTarget;
    @property(readwrite, nonatomic, unsafe_unretained) id<GPUImageInput> targetToIgnoreForUpdates;
    @property(nonatomic, copy) void(^frameProcessingCompletionBlock)(GPUImageOutput*, CMTime);
    @property(nonatomic) BOOL enabled;
    @property(readwrite, nonatomic) GPUTextureOptions outputTextureOptions;

    /// @name Managing targets
    - (void)setInputFramebufferForTarget:(id<GPUImageInput>)target atIndex:(NSInteger)inputTextureIndex;
    - (GPUImageFramebuffer *)framebufferForOutput;
    - (void)removeOutputFramebuffer;
    - (void)notifyTargetsAboutNewOutputTexture;

    /** Returns an array of the current targets.
    */
    - (NSArray*)targets;

    /** Adds a target to receive notifications when new frames are available.
    
    The target will be asked for its next available texture.
    
    See [GPUImageInput newFrameReadyAtTime:]
    
    @param newTarget Target to be added
    */
    - (void)addTarget:(id<GPUImageInput>)newTarget;

    /** Adds a target to receive notifications when new frames are available.
    
    See [GPUImageInput newFrameReadyAtTime:]
    
    @param newTarget Target to be added
    */
    - (void)addTarget:(id<GPUImageInput>)newTarget atTextureLocation:(NSInteger)textureLocation;

    /** Removes a target. The target will no longer receive notifications when new frames are available.
    
    @param targetToRemove Target to be removed
    */
    - (void)removeTarget:(id<GPUImageInput>)targetToRemove;

    /** Removes all targets.
    */
    - (void)removeAllTargets;

    /// @name Manage the output texture

    - (void)forceProcessingAtSize:(CGSize)frameSize;
    - (void)forceProcessingAtSizeRespectingAspectRatio:(CGSize)frameSize;

    /// @name Still image processing

    - (void)useNextFrameForImageCapture;
    - (CGImageRef)newCGImageFromCurrentlyProcessedOutput;
    - (CGImageRef)newCGImageByFilteringCGImage:(CGImageRef)imageToFilter;

    // Platform-specific image output methods
    // If you're trying to use these methods, remember that you need to set -useNextFrameForImageCapture before running -processImage or running video and calling any of these methods, or you will get a nil image
    #if TARGET_IPHONE_SIMULATOR || TARGET_OS_IPHONE
    - (UIImage *)imageFromCurrentFramebuffer;
    - (UIImage *)imageFromCurrentFramebufferWithOrientation:(UIImageOrientation)imageOrientation;
    - (UIImage *)imageByFilteringImage:(UIImage *)imageToFilter;
    - (CGImageRef)newCGImageByFilteringImage:(UIImage *)imageToFilter;
    #else
    - (NSImage *)imageFromCurrentFramebuffer;
    - (NSImage *)imageFromCurrentFramebufferWithOrientation:(UIImageOrientation)imageOrientation;
    - (NSImage *)imageByFilteringImage:(NSImage *)imageToFilter;
    - (CGImageRef)newCGImageByFilteringImage:(NSImage *)imageToFilter;
    #endif

    - (BOOL)providesMonochromeOutput;

1. GPUImageContext

    @property(readonly, nonatomic) dispatch_queue_t contextQueue;
    @property(readwrite, retain, nonatomic) GLProgram *currentShaderProgram;
    @property(readonly, retain, nonatomic) EAGLContext *context;
    @property(readonly) CVOpenGLESTextureCacheRef coreVideoTextureCache;
    @property(readonly) GPUImageFramebufferCache *framebufferCache;

    + (void *)contextKey;
    + (GPUImageContext *)sharedImageProcessingContext;
    + (dispatch_queue_t)sharedContextQueue;
    + (GPUImageFramebufferCache *)sharedFramebufferCache;
    + (void)useImageProcessingContext;
    - (void)useAsCurrentContext;
    + (void)setActiveShaderProgram:(GLProgram *)shaderProgram;
    - (void)setContextShaderProgram:(GLProgram *)shaderProgram;
    + (GLint)maximumTextureSizeForThisDevice;
    + (GLint)maximumTextureUnitsForThisDevice;
    + (GLint)maximumVaryingVectorsForThisDevice;
    + (BOOL)deviceSupportsOpenGLESExtension:(NSString *)extension;
    + (BOOL)deviceSupportsRedTextures;
    + (BOOL)deviceSupportsFramebufferReads;
    + (CGSize)sizeThatFitsWithinATextureForSize:(CGSize)inputSize;

    - (void)presentBufferForDisplay;
    - (GLProgram *)programForVertexShaderString:(NSString *)vertexShaderString fragmentShaderString:(NSString *)fragmentShaderString;

    - (void)useSharegroup:(EAGLSharegroup *)sharegroup;

    // Manage fast texture upload
    + (BOOL)supportsFastTextureUpload;

1. GPUImageMovieWriter
    @interface GPUImageMovieWriter : NSObject <GPUImageInput>
    {
        BOOL alreadyFinishedRecording;
        
        NSURL *movieURL;
        NSString *fileType;
        AVAssetWriter *assetWriter;
        AVAssetWriterInput *assetWriterAudioInput;
        AVAssetWriterInput *assetWriterVideoInput;
        AVAssetWriterInputPixelBufferAdaptor *assetWriterPixelBufferInput;
        
        GPUImageContext *_movieWriterContext;
        CVPixelBufferRef renderTarget;
        CVOpenGLESTextureRef renderTexture;

        CGSize videoSize;
        GPUImageRotationMode inputRotation;
    }

    @property(readwrite, nonatomic) BOOL hasAudioTrack;
    @property(readwrite, nonatomic) BOOL shouldPassthroughAudio;
    @property(readwrite, nonatomic) BOOL shouldInvalidateAudioSampleWhenDone;
    @property(nonatomic, copy) void(^completionBlock)(void);
    @property(nonatomic, copy) void(^failureBlock)(NSError*);
    @property(nonatomic, assign) id<GPUImageMovieWriterDelegate> delegate;
    @property(readwrite, nonatomic) BOOL encodingLiveVideo;
    @property(nonatomic, copy) BOOL(^videoInputReadyCallback)(void);
    @property(nonatomic, copy) BOOL(^audioInputReadyCallback)(void);
    @property(nonatomic, copy) void(^audioProcessingCallback)(SInt16 **samplesRef, CMItemCount numSamplesInBuffer);
    @property(nonatomic) BOOL enabled;
    @property(nonatomic, readonly) AVAssetWriter *assetWriter;
    @property(nonatomic, readonly) CMTime duration;
    @property(nonatomic, assign) CGAffineTransform transform;
    @property(nonatomic, copy) NSArray *metaData;
    @property(nonatomic, assign, getter = isPaused) BOOL paused;
    @property(nonatomic, retain) GPUImageContext *movieWriterContext;

    // Initialization and teardown
    - (id)initWithMovieURL:(NSURL *)newMovieURL size:(CGSize)newSize;
    - (id)initWithMovieURL:(NSURL *)newMovieURL size:(CGSize)newSize fileType:(NSString *)newFileType outputSettings:(NSDictionary *)outputSettings;

    - (void)setHasAudioTrack:(BOOL)hasAudioTrack audioSettings:(NSDictionary *)audioOutputSettings;

    // Movie recording
    - (void)startRecording;
    - (void)startRecordingInOrientation:(CGAffineTransform)orientationTransform;
    - (void)finishRecording;
    - (void)finishRecordingWithCompletionHandler:(void (^)(void))handler;
    - (void)cancelRecording;
    - (void)processAudioBuffer:(CMSampleBufferRef)audioBuffer;
    - (void)enableSynchronizationCallbacks;


1. GPUImagePicture

1. GPUImageFilterGroup

@interface GPUImageFilterGroup : GPUImageOutput <GPUImageInput>
{
    NSMutableArray *filters;
    BOOL isEndProcessing;
}

@property(readwrite, nonatomic, strong) GPUImageOutput<GPUImageInput> *terminalFilter;
@property(readwrite, nonatomic, strong) NSArray *initialFilters;
@property(readwrite, nonatomic, strong) GPUImageOutput<GPUImageInput> *inputFilterToIgnoreForUpdates; 

// Filter management
- (void)addFilter:(GPUImageOutput<GPUImageInput> *)newFilter;
- (GPUImageOutput<GPUImageInput> *)filterAtIndex:(NSUInteger)filterIndex;
- (NSUInteger)filterCount;


1. GPUImageFilterPipeline

1. GPUImageFramebuffer

1. GPUImageFramebufferCache

@interface GPUImageFramebufferCache : NSObject

// Framebuffer management
- (GPUImageFramebuffer *)fetchFramebufferForSize:(CGSize)framebufferSize textureOptions:(GPUTextureOptions)textureOptions onlyTexture:(BOOL)onlyTexture;
- (GPUImageFramebuffer *)fetchFramebufferForSize:(CGSize)framebufferSize onlyTexture:(BOOL)onlyTexture;
- (void)returnFramebufferToCache:(GPUImageFramebuffer *)framebuffer;
- (void)purgeAllUnassignedFramebuffers;
- (void)addFramebufferToActiveImageCaptureList:(GPUImageFramebuffer *)framebuffer;
- (void)removeFramebufferFromActiveImageCaptureList:(GPUImageFramebuffer *)framebuffer;

@end

1.  GPUImageMovie

@interface GPUImageMovie : GPUImageOutput

@property (readwrite, retain) AVAsset *asset;
@property (readwrite, retain) AVPlayerItem *playerItem;
@property(readwrite, retain) NSURL *url;

/** This enables the benchmarking mode, which logs out instantaneous and average frame times to the console
 */
@property(readwrite, nonatomic) BOOL runBenchmark;

/** This determines whether to play back a movie as fast as the frames can be processed, or if the original speed of the movie should be respected. Defaults to NO.
 */
@property(readwrite, nonatomic) BOOL playAtActualSpeed;

/** This determines whether the video should repeat (loop) at the end and restart from the beginning. Defaults to NO.
 */
@property(readwrite, nonatomic) BOOL shouldRepeat;

/** This specifies the progress of the process on a scale from 0 to 1.0. A value of 0 means the process has not yet begun, A value of 1.0 means the conversaion is complete.
    This property is not key-value observable.
 */
@property(readonly, nonatomic) float progress;

/** This is used to send the delete Movie did complete playing alert
 */
@property (readwrite, nonatomic, assign) id <GPUImageMovieDelegate>delegate;

@property (readonly, nonatomic) AVAssetReader *assetReader;
@property (readonly, nonatomic) BOOL audioEncodingIsFinished;
@property (readonly, nonatomic) BOOL videoEncodingIsFinished;

/// @name Initialization and teardown
- (id)initWithAsset:(AVAsset *)asset;
- (id)initWithPlayerItem:(AVPlayerItem *)playerItem;
- (id)initWithURL:(NSURL *)url;
- (void)yuvConversionSetup;

/// @name Movie processing
- (void)enableSynchronizedEncodingUsingMovieWriter:(GPUImageMovieWriter *)movieWriter;
- (BOOL)readNextVideoFrameFromOutput:(AVAssetReaderOutput *)readerVideoTrackOutput;
- (BOOL)readNextAudioSampleFromOutput:(AVAssetReaderOutput *)readerAudioTrackOutput;
- (void)startProcessing;
- (void)endProcessing;
- (void)cancelProcessing;
- (void)processMovieFrame:(CMSampleBufferRef)movieSampleBuffer; 


1. GPUImageStillCamera

    @interface GPUImageStillCamera : GPUImageVideoCamera

    /** The JPEG compression quality to use when capturing a photo as a JPEG.
    */
    @property CGFloat jpegCompressionQuality;

    // Only reliably set inside the context of the completion handler of one of the capture methods
    @property (readonly) NSDictionary *currentCaptureMetadata;

    // Photography controls
    - (void)capturePhotoAsSampleBufferWithCompletionHandler:(void (^)(CMSampleBufferRef imageSampleBuffer, NSError *error))block;
    - (void)capturePhotoAsImageProcessedUpToFilter:(GPUImageOutput<GPUImageInput> *)finalFilterInChain withCompletionHandler:(void (^)(UIImage *processedImage, NSError *error))block;
    - (void)capturePhotoAsImageProcessedUpToFilter:(GPUImageOutput<GPUImageInput> *)finalFilterInChain withOrientation:(UIImageOrientation)orientation withCompletionHandler:(void (^)(UIImage *processedImage, NSError *error))block;
    - (void)capturePhotoAsJPEGProcessedUpToFilter:(GPUImageOutput<GPUImageInput> *)finalFilterInChain withCompletionHandler:(void (^)(NSData *processedJPEG, NSError *error))block;
    - (void)capturePhotoAsJPEGProcessedUpToFilter:(GPUImageOutput<GPUImageInput> *)finalFilterInChain withOrientation:(UIImageOrientation)orientation withCompletionHandler:(void (^)(NSData *processedJPEG, NSError *error))block;
    - (void)capturePhotoAsPNGProcessedUpToFilter:(GPUImageOutput<GPUImageInput> *)finalFilterInChain withCompletionHandler:(void (^)(NSData *processedPNG, NSError *error))block;
    - (void)capturePhotoAsPNGProcessedUpToFilter:(GPUImageOutput<GPUImageInput> *)finalFilterInChain withOrientation:(UIImageOrientation)orientation withCompletionHandler:(void (^)(NSData *processedPNG, NSError *error))block;

    @end


1. GPUImageVideoCamera 

    @interface GPUImageVideoCamera : GPUImageOutput <AVCaptureVideoDataOutputSampleBufferDelegate, AVCaptureAudioDataOutputSampleBufferDelegate>
    {
        NSUInteger numberOfFramesCaptured;
        CGFloat totalFrameTimeDuringCapture;
        
        AVCaptureSession *_captureSession;
        AVCaptureDevice *_inputCamera;
        AVCaptureDevice *_microphone;
        AVCaptureDeviceInput *videoInput;
        AVCaptureVideoDataOutput *videoOutput;

        BOOL capturePaused;
        GPUImageRotationMode outputRotation, internalRotation;
        dispatch_semaphore_t frameRenderingSemaphore;
            
        BOOL captureAsYUV;
        GLuint luminanceTexture, chrominanceTexture;

        __unsafe_unretained id<GPUImageVideoCameraDelegate> _delegate;
    }

    /// The AVCaptureSession used to capture from the camera
    @property(readonly, retain, nonatomic) AVCaptureSession *captureSession;

    /// This enables the capture session preset to be changed on the fly
    @property (readwrite, nonatomic, copy) NSString *captureSessionPreset;

    /// This sets the frame rate of the camera (iOS 5 and above only)
    /**
    Setting this to 0 or below will set the frame rate back to the default setting for a particular preset.
    */
    @property (readwrite) int32_t frameRate;

    /// Easy way to tell which cameras are present on device
    @property (readonly, getter = isFrontFacingCameraPresent) BOOL frontFacingCameraPresent;
    @property (readonly, getter = isBackFacingCameraPresent) BOOL backFacingCameraPresent;

    /// This enables the benchmarking mode, which logs out instantaneous and average frame times to the console
    @property(readwrite, nonatomic) BOOL runBenchmark;

    /// Use this property to manage camera settings. Focus point, exposure point, etc.
    @property(readonly) AVCaptureDevice *inputCamera;

    /// This determines the rotation applied to the output image, based on the source material
    @property(readwrite, nonatomic) UIInterfaceOrientation outputImageOrientation;

    /// These properties determine whether or not the two camera orientations should be mirrored. By default, both are NO.
    @property(readwrite, nonatomic) BOOL horizontallyMirrorFrontFacingCamera, horizontallyMirrorRearFacingCamera;

    @property(nonatomic, assign) id<GPUImageVideoCameraDelegate> delegate;

    /// @name Initialization and teardown

    /** Begin a capture session
    
    See AVCaptureSession for acceptable values
    
    @param sessionPreset Session preset to use
    @param cameraPosition Camera to capture from
    */
    - (id)initWithSessionPreset:(NSString *)sessionPreset cameraPosition:(AVCaptureDevicePosition)cameraPosition;

    /** Add audio capture to the session. Adding inputs and outputs freezes the capture session momentarily, so you
        can use this method to add the audio inputs and outputs early, if you're going to set the audioEncodingTarget 
        later. Returns YES is the audio inputs and outputs were added, or NO if they had already been added.
    */
    - (BOOL)addAudioInputsAndOutputs;

    /** Remove the audio capture inputs and outputs from this session. Returns YES if the audio inputs and outputs
        were removed, or NO is they hadn't already been added.
    */
    - (BOOL)removeAudioInputsAndOutputs;

    /** Tear down the capture session
    */
    - (void)removeInputsAndOutputs;

    /// @name Manage the camera video stream

    /** Start camera capturing
    */
    - (void)startCameraCapture;

    /** Stop camera capturing
    */
    - (void)stopCameraCapture;

    /** Pause camera capturing
    */
    - (void)pauseCameraCapture;

    /** Resume camera capturing
    */
    - (void)resumeCameraCapture;

    /** Process a video sample
    @param sampleBuffer Buffer to process
    */
    - (void)processVideoSampleBuffer:(CMSampleBufferRef)sampleBuffer;

    /** Process an audio sample
    @param sampleBuffer Buffer to process
    */
    - (void)processAudioSampleBuffer:(CMSampleBufferRef)sampleBuffer;

    /** Get the position (front, rear) of the source camera
    */
    - (AVCaptureDevicePosition)cameraPosition;

    /** Get the AVCaptureConnection of the source camera
    */
    - (AVCaptureConnection *)videoCaptureConnection;

    /** This flips between the front and rear cameras
    */
    - (void)rotateCamera;

    /// @name Benchmarking

    /** When benchmarking is enabled, this will keep a running average of the time from uploading, processing, and final recording or display
    */
    - (CGFloat)averageFrameDurationDuringCapture;

    - (void)resetBenchmarkAverage;

    + (BOOL)isBackFacingCameraPresent;
    + (BOOL)isFrontFacingCameraPresent;

    @end

1. GPUImageView

    @interface GPUImageView : UIView <GPUImageInput>
    {
        GPUImageRotationMode inputRotation;
    }

    /** The fill mode dictates how images are fit in the view, with the default being kGPUImageFillModePreserveAspectRatio
    */
    @property(readwrite, nonatomic) GPUImageFillModeType fillMode;

    /** This calculates the current display size, in pixels, taking into account Retina scaling factors
    */
    @property(readonly, nonatomic) CGSize sizeInPixels;

    @property(nonatomic) BOOL enabled;

    /** Handling fill mode
    
    @param redComponent Red component for background color
    @param greenComponent Green component for background color
    @param blueComponent Blue component for background color
    @param alphaComponent Alpha component for background color
    */
    - (void)setBackgroundColorRed:(GLfloat)redComponent green:(GLfloat)greenComponent blue:(GLfloat)blueComponent alpha:(GLfloat)alphaComponent;

    - (void)setCurrentlyReceivingMonochromeInput:(BOOL)newValue;

    @end



## PROTOCOL

1. GPUImageInput

    @protocol GPUImageInput <NSObject>
    - (void)newFrameReadyAtTime:(CMTime)frameTime atIndex:(NSInteger)textureIndex;
    - (void)setInputFramebuffer:(GPUImageFramebuffer *)newInputFramebuffer atIndex:(NSInteger)textureIndex;
    - (NSInteger)nextAvailableTextureIndex;
    - (void)setInputSize:(CGSize)newSize atIndex:(NSInteger)textureIndex;
    - (void)setInputRotation:(GPUImageRotationMode)newInputRotation atIndex:(NSInteger)textureIndex;
    - (CGSize)maximumOutputSize;
    - (void)endProcessing;
    - (BOOL)shouldIgnoreUpdatesToThisTarget;
    - (BOOL)enabled;
    - (BOOL)wantsMonochromeInput;
    - (void)setCurrentlyReceivingMonochromeInput:(BOOL)newValue;
    @end